{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JCMdYUfar-Ej6mH_UPd_I7pFJumXapMa","timestamp":1682062137744},{"file_id":"https://github.com/Shinchan9913/FlaskTut1/blob/main/Work_1.ipynb","timestamp":1681885094122}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **Importing necessary libraries for building the model**"],"metadata":{"id":"zJmRD5Vcla4-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cw_nknsYwpPY"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["## **Mounting Google Drive to access files**"],"metadata":{"id":"OmJJ2FeXl1kG"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkVyKq6ow5Ji","outputId":"306b33e3-7897-49b1-a8d2-f703da8a8ef2","executionInfo":{"status":"ok","timestamp":1682007807405,"user_tz":-330,"elapsed":29754,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"hTwSVp02l68S"}},{"cell_type":"markdown","source":["# **Dataset**\n","Set up Kaggle API key and download datasets."],"metadata":{"id":"krvDO665mR1k"}},{"cell_type":"code","source":["!mkdir ~/.kaggle\n","!cp /content/drive/MyDrive/Colab\\ Notebooks/Kaggle/kaggle.json ~/.kaggle\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle datasets download kazanova/sentiment140\n","!unzip sentiment140.zip\n","!kaggle datasets download yasserh/imdb-movie-ratings-sentiment-analysis\n","!unzip imdb-movie-ratings-sentiment-analysis.zip\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIcCNTwPx0-Z","outputId":"ab6554be-185b-4f5c-b1df-ab0c18983d1f","executionInfo":{"status":"ok","timestamp":1682007818894,"user_tz":-330,"elapsed":7427,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading sentiment140.zip to /content\n"," 77% 62.0M/80.9M [00:00<00:00, 175MB/s]\n","100% 80.9M/80.9M [00:00<00:00, 177MB/s]\n","Archive:  sentiment140.zip\n","  inflating: training.1600000.processed.noemoticon.csv  \n","Downloading imdb-movie-ratings-sentiment-analysis.zip to /content\n"," 49% 10.0M/20.6M [00:00<00:00, 26.5MB/s]\n","100% 20.6M/20.6M [00:00<00:00, 47.6MB/s]\n","Archive:  imdb-movie-ratings-sentiment-analysis.zip\n","  inflating: movie.csv               \n"]}]},{"cell_type":"markdown","source":["Importing pandas library and loading datasets."],"metadata":{"id":"7dMTnUgLmmec"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', header=None, names=['target', 'id', 'date', 'flag', 'user', 'text'], encoding='latin-1')\n","df = df[['target', 'text']]\n","\n","dfo = pd.read_csv('/content/movie.csv')\n","# df.head(5)\n","\n","dfo=dfo.rename(columns={'text': 'target', 'label': 'text'})\n","dfo['text'], dfo['target'] = dfo['target'], dfo['text'] \n","\n","df = pd.concat([df, dfo], ignore_index=True)\n","\n","# dfo.head(5)"],"metadata":{"id":"xrSk2D-xyJT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dfo"],"metadata":{"id":"FlOg86lFOsA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ZEOuo-R0mtl1"}},{"cell_type":"markdown","source":["## Setting up the parameters for the neural network:\n","* vocabulary size\n","* embedding dimension\n","* maximum length of a sequence\n","* truncation and padding types\n","* out-of-vocabulary token\n","* size of the training set."],"metadata":{"id":"Qm70QtNVm94d"}},{"cell_type":"code","source":["vocab_size = 20000\n","embedding_dim = 32\n","max_length = 100\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size = 20000"],"metadata":{"id":"j9Hr7U0yySCy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Importing the Natural Language Toolkit (NLTK) library for natural language processing\n","* Downloading the stopwords and wordnet corpora using NLTK\n","* Unzipping the wordnet corpus to the appropriate directory."],"metadata":{"id":"KWxlaKjtnOB1"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7plVoHW0OrSz","outputId":"a48f9c45-2254-46ba-a465-4d66987c99c1","executionInfo":{"status":"ok","timestamp":1682007822243,"user_tz":-330,"elapsed":1424,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Archive:  /root/nltk_data/corpora/wordnet.zip\n","   creating: /root/nltk_data/corpora/wordnet/\n","  inflating: /root/nltk_data/corpora/wordnet/lexnames  \n","  inflating: /root/nltk_data/corpora/wordnet/data.verb  \n","  inflating: /root/nltk_data/corpora/wordnet/index.adv  \n","  inflating: /root/nltk_data/corpora/wordnet/adv.exc  \n","  inflating: /root/nltk_data/corpora/wordnet/index.verb  \n","  inflating: /root/nltk_data/corpora/wordnet/cntlist.rev  \n","  inflating: /root/nltk_data/corpora/wordnet/data.adj  \n","  inflating: /root/nltk_data/corpora/wordnet/index.adj  \n","  inflating: /root/nltk_data/corpora/wordnet/LICENSE  \n","  inflating: /root/nltk_data/corpora/wordnet/citation.bib  \n","  inflating: /root/nltk_data/corpora/wordnet/noun.exc  \n","  inflating: /root/nltk_data/corpora/wordnet/verb.exc  \n","  inflating: /root/nltk_data/corpora/wordnet/README  \n","  inflating: /root/nltk_data/corpora/wordnet/index.sense  \n","  inflating: /root/nltk_data/corpora/wordnet/data.noun  \n","  inflating: /root/nltk_data/corpora/wordnet/data.adv  \n","  inflating: /root/nltk_data/corpora/wordnet/index.noun  \n","  inflating: /root/nltk_data/corpora/wordnet/adj.exc  \n"]}]},{"cell_type":"code","source":["# df"],"metadata":{"id":"yfypY2bbH_EL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* The function preprocess() takes in a text as input.\n","* It preprocesses the text by:\n"," * Replacing repeated punctuation signs with labels and adding spaces.\n"," * Adding spaces before and after single punctuation signs.\n"," * Lowercasing the text.\n"," * Removing stopwords.\n"," * Lemmatizing the text.\n","* The function returns the preprocessed text."],"metadata":{"id":"gYSvVW2mnlZB"}},{"cell_type":"code","source":["import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","def preprocess(text):\n","    text = ' '.join(text)\n","    # Replace repeated punctuation signs with labels and add spaces\n","    text = re.sub(r'(\\.{2,})', r' multistop ', text)\n","    text = re.sub(r'(\\!{2,})', r' multiexclamation ', text)\n","    text = re.sub(r'(\\?{2,})', r' multiquestion ', text)\n","    # Add spaces before and after single punctuation signs\n","    text = re.sub(r'(\\.|\\!|\\?|\\,)', r' ', text)\n","    \n","    # Lower case the text\n","    text = text.lower()\n","    \n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    words = text.split()\n","    words = [w for w in words if not w in stop_words]\n","    text = ' '.join(words)\n","    \n","    # Lemmatize text\n","    lemmatizer = WordNetLemmatizer()\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(w) for w in words]\n","    text = ' '.join(words)\n","\n","    return text\n","\n"],"metadata":{"id":"Dr0dvBDGPELg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df[\"text\"] = df[\"text\"].apply(preprocess)\n","\n","# df.head()"],"metadata":{"id":"HWdeNmCvA8wE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Function: load_emoticons(emo_filename)**\n","\n","* Load emoticons and their polarity from a file\n","* Parameters:\n"," * emo_filename: path to the emoticon file\n","* Returns:\n"," * A dictionary containing the emoticons as keys and their polarity as values\n","\n","## **Function: replace_emoticons(text, emoticon_dict=emoticon_dict)**\n","\n","* Replace emoticons with their polarity and delete neutral ones\n","* Parameters:\n"," * text: input text containing emoticons\n"," * emoticon_dict: dictionary containing emoticons as keys and their polarity as values\n","* Returns:\n"," * A list of words obtained after replacing emoticons with their polarity and deleting neutral ones"],"metadata":{"id":"5OgGTdgRoBR9"}},{"cell_type":"code","source":["def load_emoticons(emo_filename):\n","    # Load emoticons and their polarity from a file\n","    emoticon_dict = {}\n","    with open(emo_filename, 'r', encoding='latin-1') as file:\n","        for line in file:\n","            emoticon, polarity = line.strip().split('\\t')\n","            emoticon_dict[emoticon] = polarity\n","    return emoticon_dict\n","\n","# Load emoticons and their polarity from a file\n","emoticon_dict = load_emoticons('/content/EmoticonLookupTable.txt')\n","\n","def replace_emoticons(text, emoticon_dict=emoticon_dict):\n","    # Replace emoticons with their polarity and delete neutral ones\n","    for emoticon, polarity in emoticon_dict.items():\n","        pattern = re.compile(re.escape(emoticon), re.IGNORECASE)\n","        if polarity == '1':\n","            text = pattern.sub(\"positive\", text)\n","        elif polarity == '-1':\n","            text = pattern.sub(\"negative\", text)\n","        else:\n","            text = pattern.sub('', text)\n","            \n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","            \n","    return text.split()\n","\n"],"metadata":{"id":"fgsHw_XVR2si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df[\"text\"] = df[\"text\"].apply(replace_emoticons)\n","# df.head()"],"metadata":{"id":"CP5jkdwYA_AH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* load_slang(slang_filename): loads a slang dictionary from a file and returns it as a Python dictionary.\n","* replace_slang(tokens, slang_dict=slang_dict): replaces slang words in a list of tokens with their corresponding meanings using the slang dictionary.\n"],"metadata":{"id":"dE4POHztogUC"}},{"cell_type":"code","source":["def load_slang(slang_filename):\n","    # Load emoticons and their polarity from a file\n","    slang_dict = {}\n","    with open(slang_filename, 'r', encoding='latin-1') as file:\n","        for line in file:\n","            slang, meaning = line.strip().split('\\t')\n","            slang_dict[slang] = meaning\n","    return slang_dict\n","\n","# Load emoticons and their polarity from a file\n","slang_dict = load_slang('/content/SlangLookupTable.txt')\n","\n","def replace_slang(tokens, slang_dict=slang_dict):\n","    # Replace emoticons with their polarity and delete neutral ones\n","    for i, token in enumerate(tokens):\n","        if token in slang_dict:\n","            tokens[i] = slang_dict[token]\n","            \n","    return tokens\n","\n"],"metadata":{"id":"YTkfHIJYTJuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df[\"text\"] = df[\"text\"].apply(replace_slang)\n","# df.head()"],"metadata":{"id":"V-QO98pdBBEM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Code preprocesses the input text by labeling user mentions, hashtags, and URLs as PERSON, TOPIC, and URL respectively."],"metadata":{"id":"7Wc4tRMMovK6"}},{"cell_type":"code","source":["def label_user_topic(tokens):\n","    labeled_tokens = []\n","    for token in tokens:\n","        if token.startswith(\"@\"):\n","            labeled_tokens.append(\"PERSON\")\n","        elif token.startswith(\"#\"):\n","            labeled_tokens.append(\"TOPIC\")\n","        elif token.startswith(\"http\"):\n","            labeled_tokens.append(\"URL\")\n","        else:\n","            labeled_tokens.append(token)\n","    return labeled_tokens\n","\n"],"metadata":{"id":"KYhwDZxQTO-m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df['text'] = df['text'].apply(label_user_topic)\n","# df.head()"],"metadata":{"id":"elTbkgEmBDij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines functions to reduce words to their base form and normalize words in a list of tokens. The reduce_word function checks if a word is in Roget's Thesaurus, and if not, iteratively removes repeated letters until it finds a match. The normalize_words function applies reduce_word to each word in a list of tokens and returns the normalized list."],"metadata":{"id":"sQrllY8qo4uV"}},{"cell_type":"code","source":["from nltk.corpus import wordnet as wn\n","\n","def reduce_word(word):\n","    # Check if the word is in Roget's Thesaurus\n","    synsets = wn.synsets(word)\n","    if synsets:\n","        return word\n","    \n","    # Iterate over the letters in the word, starting from the end\n","    for i in range(len(word)-1, 1, -1):\n","        # If the current letter is the same as the previous one,\n","        # remove the current letter and check if the resulting word\n","        # is in Roget's Thesaurus\n","        if word[i] == word[i-1]:\n","            word = word[:i] + word[i+1:]\n","            synsets = wn.synsets(word)\n","            if synsets:\n","                return \"STRESSED \" + word\n","\n","        # If the current and previous letters are the same as the one before them,\n","        # remove the current letter and check if the resulting word\n","        # is in Roget's Thesaurus\n","        elif i > 2 and word[i] == word[i-2]:\n","            word = word[:i-1] + word[i:]\n","            synsets = wn.synsets(word)\n","            if synsets:\n","                return \"STRESSED \" + word\n","    \n","    # If no match is found, return the original word\n","    return word\n","\n","\n","def normalize_words(tokens):\n","    normalized_tokens = []\n","    for token in tokens:\n","        # Check if the token is a word\n","        if re.match(r'\\b\\w+\\b', token):\n","            # Normalize the word\n","            normalized_word = reduce_word(token.lower())\n","            # If the normalized word is different from the original word,\n","            # add both versions to the list of tokens\n","            if normalized_word != token.lower():\n","                normalized_tokens.append(normalized_word)\n","            else:\n","                normalized_tokens.append(token)\n","        else:\n","            normalized_tokens.append(token)\n","            \n","#     normalized_tokens = [token.split() if 'STRESSED' in token else token for token in normalized_tokens]\n","#     normalized_tokens = [item if not isinstance(item, list) else item for sublist in normalized_tokens for item in sublist]\n","    return normalized_tokens\n"],"metadata":{"id":"YdmjzbCdTZVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# df[\"text\"] = df[\"text\"].apply(normalize_words)\n","# df.tail()"],"metadata":{"id":"c1mV61YuBGNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install the twython package and downloads the vader_lexicon, opinion_lexicon, and sentiwordnet resources from NLTK."],"metadata":{"id":"0RNAkq-3pFeN"}},{"cell_type":"code","source":["!pip install twython\n","nltk.download('vader_lexicon')\n","nltk.download('opinion_lexicon')\n","nltk.download('sentiwordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TdKIsVrTtLJ","outputId":"ca23d8ca-18a5-4ff1-b0d6-9fa1ccad10ef","executionInfo":{"status":"ok","timestamp":1682007846020,"user_tz":-330,"elapsed":5647,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting twython\n","  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from twython) (2.27.1)\n","Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from twython) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython) (2.0.12)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.2.2)\n","Installing collected packages: twython\n","Successfully installed twython-3.9.1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["This function matches affect words to tokens and returns the tokens with affect labels."],"metadata":{"id":"56M3X5NHpKrS"}},{"cell_type":"code","source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","from nltk.corpus import opinion_lexicon, sentiwordnet\n","\n","\n","def match_affect_words(tokens):\n","    sia = SentimentIntensityAnalyzer()\n","    positive_words = set(opinion_lexicon.positive())\n","    negative_words = set(opinion_lexicon.negative())\n","    hpositive_words = set(word for synset in sentiwordnet.senti_synsets('', 'a') for word, pos in synset.lemmas() \n","                          if synset.pos_score() > 0.75 and word not in positive_words)\n","    hnegative_words = set(word for synset in sentiwordnet.senti_synsets('', 'a') for word, pos in synset.lemmas() \n","                          if synset.neg_score() > 0.75 and word not in negative_words)\n","    affect_labels = {'positive': positive_words, 'negative': negative_words, \n","                     'hpositive': hpositive_words, 'hnegative': hnegative_words}\n","    \n","    # create a mapping from affect words to labels\n","    affect_words = set(word for label_words in affect_labels.values() for word in label_words)\n","    word_to_label = {}\n","    for word in affect_words:\n","        scores = sia.polarity_scores(word)\n","        if scores['compound'] >= 0.5:\n","            word_to_label[word] = 'hpositive'\n","        elif scores['compound'] > -0.5 and scores['compound'] < 0.5:\n","            word_to_label[word] = 'positive' if word in positive_words else 'negative'\n","        else:\n","            word_to_label[word] = 'hnegative'\n","    \n","    # match tokens to affect labels\n","    affect_set = set(affect_words)\n","    matched_tokens = []\n","    for token in tokens:\n","        if token in affect_set:\n","            label = word_to_label[token]\n","            matched_tokens.append(label)\n","        else:\n","            matched_tokens.append(token)\n","    \n","    return matched_tokens\n","\n"],"metadata":{"id":"rDrkAdbETziG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df[\"text\"] = df[\"text\"].apply(match_affect_words)\n","# df.tail()"],"metadata":{"id":"M0soad1vh3Ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function matches modifier words in a list of tokens and returns a new list with the matched tokens and their corresponding modifier labels: \"negator\", \"intensifier\", or \"diminisher\". The modifier word lists are defined at the beginning of the function."],"metadata":{"id":"zlW_DELEpO-U"}},{"cell_type":"code","source":["# define the lists of negation, intensification and diminishment expressions\n","negation_list = [\"no\", \"not\", \"never\", \"none\", \"nobody\", \"nowhere\", \"nothing\", \"neither\", \"nor\", \"cannot\", \"can't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shouldn't\", \"couldn't\", \"isn't\", \"aren't\", \"ain't\", \"hate\", \"dislike\", \"disapprove\", \"disapprove of\", \"disagree\", \"disagree with\", \"reject\", \"rejects\", \"rejected\", \"refuse\", \"refuses\", \"refused\", \"never\", \"rarely\", \"seldom\", \"hardly\", \"scarcely\", \"barely\"]\n","intensification_list = [\"very\", \"extremely\", \"super\", \"really\", \"quite\", \"most\", \"more\", \"quite\", \"too\", \"enough\", \"so\", \"such\", \"just\", \"almost\", \"absolutely\", \"completely\", \"totally\", \"utterly\", \"highly\", \"deeply\", \"greatly\", \"seriously\", \"intensely\", \"especially\", \"exceedingly\", \"exceptionally\", \"particularly\", \"unusually\", \"incredibly\", \"undeniably\", \"undeniable\", \"emphatically\", \"decidedly\", \"really\", \"truly\", \"hugely\", \"mega\", \"ultra\", \"majorly\", \"extraordinarily\", \"mightily\", \"fully\", \"mightily\", \"perfectly\", \"thoroughly\", \"utterly\", \"all\", \"way\", \"significantly\", \"terribly\", \"awfully\", \"fantastically\"]\n","diminishment_list = [\"little\", \"slightly\", \"somewhat\", \"kind\", \"sort\", \"bit\", \"little\", \"moderately\", \"marginally\", \"fairly\", \"reasonably\", \"comparatively\", \"relatively\", \"tad\", \"touch\", \"extent\"]\n","\n","\n","def match_modifier_words(tokens):\n","    matched_tokens = []\n","    for token in tokens:\n","        if token in negation_list:\n","            matched_tokens.append(\"negator\")\n","        elif token in intensification_list:\n","            matched_tokens.append(\"intensifier\")\n","        elif token in diminishment_list:\n","            matched_tokens.append(\"diminisher\")\n","        else:\n","            matched_tokens.append(token)\n","    return matched_tokens\n"],"metadata":{"id":"JpFnY_naUGEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# df[\"text\"] = df[\"text\"].apply(match_modifier_words)\n","# df.tail()"],"metadata":{"id":"0gQe8-MoBJdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Converting all text to lowercase\n","* Splitting text into tokens\n","* Matching modifier words (negation, intensification, and diminishment expressions) and labeling them accordingly\n","* Applying additional preprocessing steps, including replacing emoticons and slang, labeling user and topic mentions, and normalizing words\n","* Returning the preprocessed DataFrame"],"metadata":{"id":"RO-niX_DpULR"}},{"cell_type":"code","source":["def process2(df):\n","    df['preprocessed_text'] = df['text'].str.lower()\n","    df['preprocessed_text'] = df['text'].str.split()\n","    text = df['preprocessed_text'].copy()\n","    text = text.apply(match_modifier_words)\n","    text = text.apply(preprocess)\n","    text = text.apply(replace_emoticons)\n","    text = text.apply(replace_slang)\n","    text = text.apply(label_user_topic)\n","    text = text.apply(normalize_words)\n","    df['preprocessed_text'] = text\n","\n","    return df"],"metadata":{"id":"_WoAmgpODe4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df.columns = \n","# dfo.index = pd.Index(range(len(dfo)))\n","# print(dfo)\n","df = process2(df)\n","# dfo = dfo.rename(columns={0: 'target', 1: 'text'})\n","# dfo['text']"],"metadata":{"id":"WQ1eP74QUNMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data = df\n","df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: ' '.join(x))\n","# df = data\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uSW3Y9KrOJTa","executionInfo":{"status":"ok","timestamp":1681910663540,"user_tz":-330,"elapsed":924,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}},"outputId":"3872930b-17cc-40e4-ef61-e37645ecfd7e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   target                                               text  \\\n","0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n","1       0  is upset that he can't update his Facebook by ...   \n","2       0  @Kenichan I dived many times for the ball. Man...   \n","3       0    my whole body feels itchy and like its on fire    \n","4       0  @nationwideclass no, it's not behaving at all....   \n","\n","                                   preprocessed_text  \n","0  switchfot URL comyzl aw thats bummer shoulda g...  \n","1  upset negator update facebok texting multistop...  \n","2  kenichan dived many time ball managed save res...  \n","3                    whole body feel itchy like fire  \n","4  nationwideclas negator behaving im mad negator...  "],"text/html":["\n","  <div id=\"df-f1a85d4c-2060-4e62-b484-4041bec2faf9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","      <th>preprocessed_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","      <td>switchfot URL comyzl aw thats bummer shoulda g...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","      <td>upset negator update facebok texting multistop...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","      <td>kenichan dived many time ball managed save res...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","      <td>whole body feel itchy like fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","      <td>nationwideclas negator behaving im mad negator...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1a85d4c-2060-4e62-b484-4041bec2faf9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f1a85d4c-2060-4e62-b484-4041bec2faf9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f1a85d4c-2060-4e62-b484-4041bec2faf9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Save to the .csv file"],"metadata":{"id":"lflP-Vibpb0s"}},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/processed_comment.csv', index=False)"],"metadata":{"id":"VhjBfrecsXbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code is splitting a dataframe into train and test sets for a binary classification problem where the target variable is being converted from a range of [0, 4] to [0, 1]. The train_test_split function from sklearn is used for this purpose."],"metadata":{"id":"I7w2Ikw2pmAA"}},{"cell_type":"code","source":["X = df[\"preprocessed_text\"]\n","y = df[\"target\"]\n","\n","y = y.replace(4,1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"],"metadata":{"id":"HHhAIjhKyXOt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creates a tokenizer object with a vocabulary size of vocab_size and an out-of-vocabulary token of oov_tok."],"metadata":{"id":"VU_9fp_tppj8"}},{"cell_type":"code","source":["tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n"],"metadata":{"id":"9VzAhu4Iyi-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.fit_on_texts(X_train)\n","word_index = tokenizer.word_index"],"metadata":{"id":"ng3Wq2pRffU-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These lines of code use the Tokenizer object to fit on the preprocessed training text and generate a word index. The fit_on_texts method updates the internal vocabulary based on the frequency of each word in the training set, while the word_index attribute returns a dictionary containing the unique words in the training set as keys and their corresponding index as values."],"metadata":{"id":"54--_I8tpuQp"}},{"cell_type":"markdown","source":["We use the Tokenizer class to tokenize the training and testing data. First, we fit the tokenizer on the training data using tokenizer.fit_on_texts(X_train), which builds the vocabulary from the training text. Then, we use the tokenizer to convert the text to sequences using tokenizer.texts_to_sequences(), and pad the sequences to a fixed length using pad_sequences(). This creates numerical inputs that can be fed into a neural network for classification."],"metadata":{"id":"Yj-OUCEOp5gH"}},{"cell_type":"code","source":["X_train_sequences = tokenizer.texts_to_sequences(X_train)\n","X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","X_test_sequences = tokenizer.texts_to_sequences(X_test)\n","X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"],"metadata":{"id":"EO0M5Keryl5r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines a sequential model in Keras with an embedding layer, two bidirectional LSTM layers, two dense layers, and a dropout layer. The model is then compiled with binary crossentropy loss, Adam optimizer, and accuracy metric."],"metadata":{"id":"zvcPWjlZqB10"}},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compiling the module\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"j4C8yxE9ypqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code trains the model for 3 epochs with a batch size of 16, using the preprocessed training data and validation data. The training progress is stored in the history variable."],"metadata":{"id":"BK36BRBIqGg2"}},{"cell_type":"code","source":["num_epochs = 3\n","history = model.fit(X_train_padded, y_train, epochs=num_epochs,batch_size=16, validation_data=(X_test_padded, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cII9LMCvyxnu","outputId":"87305553-13ff-45f9-c234-c523047372a6","executionInfo":{"status":"ok","timestamp":1681915851761,"user_tz":-330,"elapsed":2629719,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","82000/82000 [==============================] - 1670s 20ms/step - loss: 0.4366 - accuracy: 0.7991 - val_loss: 0.4145 - val_accuracy: 0.8104\n","Epoch 2/3\n","82000/82000 [==============================] - 1592s 19ms/step - loss: 0.4032 - accuracy: 0.8183 - val_loss: 0.4034 - val_accuracy: 0.8155\n","Epoch 3/3\n","82000/82000 [==============================] - 1598s 19ms/step - loss: 0.3890 - accuracy: 0.8263 - val_loss: 0.4057 - val_accuracy: 0.8164\n"]}]},{"cell_type":"markdown","source":["The code below evaluates the trained model using the test set and prints the test accuracy."],"metadata":{"id":"0D8j4-fyqPeg"}},{"cell_type":"code","source":["test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=2)\n","print(\"Test Accuracy: \", test_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPsNJqmmy1r9","outputId":"960042e7-2b76-4bbe-b968-4ad2d2d5f3ea","executionInfo":{"status":"ok","timestamp":1681916050721,"user_tz":-330,"elapsed":144351,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10250/10250 - 84s - loss: 0.4057 - accuracy: 0.8164 - 84s/epoch - 8ms/step\n","Test Accuracy:  0.8164024353027344\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1oGqKg2XK13x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test with input entry\n","entry = [\"I'm not sure if I don't like it or if I just don't understand it.\"]\n","test_text = tokenizer.texts_to_sequences(entry)\n","test_text_padded = pad_sequences(test_text, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","prediction = model.predict(test_text_padded)\n","\n","#0 : bad\n","#1 : good\n","print(prediction)\n","\n","# entry = ['I hate this music so bad ! I just want to sleep rn']\n","# test_text = tokenizer.texts_to_sequences(entry)\n","# test_text_padded = pad_sequences(test_text, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","# prediction = model.predict(test_text_padded)\n","\n","\n","# #0 : bad\n","# #1 : good\n","# print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYPgMj1ty5wE","outputId":"8087d5c6-70da-4f9c-d37a-0c000947480c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step\n","[[0.468318]]\n"]}]},{"cell_type":"markdown","source":["Saving the model as a .keras file"],"metadata":{"id":"JbipTSshqUz-"}},{"cell_type":"code","source":["model.save('/content/drive/MyDrive/Colab Notebooks/model.keras')"],"metadata":{"id":"iaBv5KAqzFjt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading the saved model"],"metadata":{"id":"4QTvUgTiqZGV"}},{"cell_type":"code","source":["load_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/model.keras')\n","\n","# Show the model architecture\n","load_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qv0DvBnX_KkT","outputId":"9e804919-088f-4ec8-f58b-73295c10f4dc","executionInfo":{"status":"ok","timestamp":1682007906655,"user_tz":-330,"elapsed":6912,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 100, 32)           640000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         49664     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 64)                4160      \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 735,105\n","Trainable params: 735,105\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Test with input entry\n","entry = [\"I disagree with some of the points made in this video.\"]\n","tokenizer.fit_on_texts(entry)\n","test_text = tokenizer.texts_to_sequences(entry)\n","test_text_padded = pad_sequences(test_text, maxlen=max_length, padding=padding_type, truncating='post')\n","\n","prediction = load_model.predict(test_text_padded)\n","\n","#0 : bad\n","#1 : good\n","print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObWkEfmmexC7","outputId":"aaf991e7-8326-44fa-9f7f-9b9f44e8e8f5","executionInfo":{"status":"ok","timestamp":1682008117163,"user_tz":-330,"elapsed":5,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 35ms/step\n","[[0.3351707]]\n"]}]},{"cell_type":"markdown","source":["Define a preprocess function for dataframe input"],"metadata":{"id":"AVRXq-Ooqc1V"}},{"cell_type":"code","source":["def process(df):\n","    df[\"text\"] = df[\"text\"].apply(match_modifier_words)\n","    df[\"text\"] = df[\"text\"].apply(preprocess)\n","    df[\"text\"] = df[\"text\"].apply(replace_emoticons)\n","    df[\"text\"] = df[\"text\"].apply(replace_slang)\n","    df['text'] = df['text'].apply(label_user_topic)\n","    df[\"text\"] = df[\"text\"].apply(normalize_words)\n","    \n","    return df"],"metadata":{"id":"QcuSJllT_0aP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define a preprocess function for text input"],"metadata":{"id":"0pF1ragFqgvO"}},{"cell_type":"code","source":["def p_text(text):\n","  text = text.lower()\n","  text = text.split()\n","  text = match_modifier_words(text)\n","  text = preprocess(text)\n","  text = replace_emoticons(text)\n","  text = replace_slang(text)\n","  text = label_user_topic(text)\n","  text = normalize_words(text)\n","  text = ' '.join(text)\n","  entry = [text]\n","  print(entry)\n","  tokenizer.fit_on_texts(entry)\n","  test_text = tokenizer.texts_to_sequences(entry)\n","  test_text_padded = pad_sequences(test_text, maxlen=max_length, padding=padding_type, truncating='post')\n","  print(test_text_padded)\n","  prediction = load_model.predict(test_text_padded)\n","  return prediction\n","\n","print(p_text(\"good work, keep it up!\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CEMTQTYTp_m","executionInfo":{"status":"ok","timestamp":1681933111008,"user_tz":-330,"elapsed":2,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}},"outputId":"6006427f-65fe-44bc-b3c9-90fa476c42b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2 4 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","1/1 [==============================] - 0s 22ms/step\n","[[0.19152078]]\n"]}]},{"cell_type":"markdown","source":["Connect to Youtube API and save comments to a .csv file"],"metadata":{"id":"5BQKqLbgqkpY"}},{"cell_type":"code","source":["from googleapiclient.discovery import build\n","import pandas as pd\n","import csv\n","!pip install pytube\n","from pytube import extract\n","\n","api_key = 'AIzaSyAWvcYyhifqwreIpseWyyFGMljWOEbO0lI'\n","\n","\n","\n","number = 0\n","comm=[]\n","\n","def video_comments(url):\n","\n","  video_id = extract.video_id(url)\n","\n","  comment_count = 0\n","\n","  youtube = build('youtube', 'v3', developerKey=api_key)\n","\n","  video_response = youtube.commentThreads().list(\n","  part = 'snippet,replies',\n","  videoId = video_id\n","  ).execute()\n","\n","  while video_response:\n","    for item in video_response['items']:\n","    \n","      comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n","      arr=[comment]\n","      comm.append(arr)\n","\n","      comment_count += 1\n","\n","    if 'nextPageToken' in video_response:\n","      video_response = youtube.commentThreads().list(\n","          part = 'snippet,replies',\n","          videoId = video_id,\n","          pageToken = video_response['nextPageToken']\n","        ).execute()\n","    else:\n","        break\n","  with open('/content/comment.csv', 'w', newline='') as filee:\n","    writer = csv.writer(filee)\n","    writer.writerow([\"Comments\"])\n","    writer.writerows(comm)\n","  filee.close();\n","#video_id = \"iEqYnkhro8E\"\n","# url = input(\"Enter Youtube video url \\n: \")\n","video_id = extract.video_id(\"https://www.youtube.com/watch?v=qHdidWEuyVI\")\n","\n","video_comments('https://www.youtube.com/watch?v=qHdidWEuyVI')"],"metadata":{"id":"gR6Sn9FtmMiy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681896355407,"user_tz":-330,"elapsed":12368,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}},"outputId":"aad3e7ff-0547-407d-d21e-650bbb0d5e76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytube\n","  Downloading pytube-12.1.3-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytube\n","Successfully installed pytube-12.1.3\n"]}]},{"cell_type":"code","source":["# import csv\n","\n","# # Open the CSV file\n","# with open('comment.csv', mode='r') as infile:\n","#     reader = csv.reader(infile)\n","#     rows = list(reader)\n","\n","# # Change the header\n","# rows[0] = ['text']\n","\n","# # Write the new header to the CSV file\n","# with open('comment.csv', mode='w', newline='') as outfile:\n","#     writer = csv.writer(outfile)\n","#     writer.writerows(rows)"],"metadata":{"id":"hsfavlCMAS2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = pd.read_csv('comment.csv')\n","df2.head()\n","df2.columns = ['text']\n","df2 = df2.reset_index(drop=True)\n","# apply preprocess function to text column and store the result in new column\n","df2['preprocessed_text'] = process2(df2)\n","df2['preprocessed_text'] = df2['preprocessed_text'].apply(lambda x: ' '.join(x))\n","# save the updated dataframe to csv\n","df2.to_csv('comment.csv', index=False)"],"metadata":{"id":"N7FD0nRlpkZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/model.keras')\n","\n","# Show the model architecture\n","load_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlwHjQQKCeZ0","executionInfo":{"status":"ok","timestamp":1681931540368,"user_tz":-330,"elapsed":3492,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}},"outputId":"cc3fcba2-e3f9-491d-c6b7-26515d9818cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 100, 32)           640000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         49664     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               41216     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 64)                4160      \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 735,105\n","Trainable params: 735,105\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["comments = df2['preprocessed_text']\n","tokenizer.fit_on_texts(comments)\n","test_text = tokenizer.texts_to_sequences(comments)\n","test_text_padded = pad_sequences(test_text, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","prediction = load_model.predict(test_text_padded)\n","\n","df2['prediction'] = prediction\n","# function to replace values greater than 0.5 with 1 and others with 0\n","replace_func = lambda x: 1 if x > 0.5 else 0\n","\n","# apply the function to the column and store the result in a new column\n","df2['sentiment'] = df2['prediction'].apply(replace_func)\n","df2.to_csv('comment.csv', index=False)\n","print(df2['sentiment'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjkrZuJ6CxfY","executionInfo":{"status":"ok","timestamp":1681896473010,"user_tz":-330,"elapsed":3118,"user":{"displayName":"SARISH VIKRANT NILAKHE","userId":"07502855197773445584"}},"outputId":"ac78d050-56b6-47ad-8557-5387a93da863"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 2s 49ms/step\n","0      0\n","1      0\n","2      1\n","3      0\n","4      0\n","      ..\n","248    0\n","249    0\n","250    0\n","251    1\n","252    1\n","Name: sentiment, Length: 253, dtype: int64\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zqRY9bT-E40R"},"execution_count":null,"outputs":[]}]}